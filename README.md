# LLMs
Add papers regarding LLMs as well as some of the prompt engineering tricks and tips


### ALiBi
- Postional embeddings are removed, hence we can extrapolate to any length theroetically
- Fixed set of bias are added instead of positional embeddings and even though we can extrapolate to more than tokens we trained. It can still consider only till tokens which it was trained as context on during inference.


